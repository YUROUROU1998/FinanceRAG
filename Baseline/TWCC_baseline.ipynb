{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905ba526",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bce125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in ./.local/lib/python3.12/site-packages (5.1.2)\n",
      "Requirement already satisfied: datasets in ./.local/lib/python3.12/site-packages (4.4.1)\n",
      "Requirement already satisfied: pytrec_eval in ./.local/lib/python3.12/site-packages (0.5)\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: pyarrow in ./.local/lib/python3.12/site-packages (22.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.local/lib/python3.12/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.local/lib/python3.12/site-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.local/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.14.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.local/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in ./.local/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (79.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.9.86)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#TWCC專用\n",
    "%pip install sentence-transformers datasets pytrec_eval accelerate pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c476acb8-bc6b-4752-b754-0a5bbc70cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.5.1+cu121\n",
      "Uninstalling torch-2.5.1+cu121:\n",
      "  Successfully uninstalled torch-2.5.1+cu121\n",
      "Found existing installation: torchvision 0.20.1+cu121\n",
      "Uninstalling torchvision-0.20.1+cu121:\n",
      "  Successfully uninstalled torchvision-0.20.1+cu121\n",
      "Found existing installation: torchaudio 2.5.1+cu121\n",
      "Uninstalling torchaudio-2.5.1+cu121:\n",
      "  Successfully uninstalled torchaudio-2.5.1+cu121\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0a0+34c6371d24.nv25.8)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0a0+428a54c9)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (79.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.4.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.4.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.2.2%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.2.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "INFO: pip is still looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.12/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.12/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.12/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.12/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.12/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.12/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.9.86)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [torchaudio]3\u001b[0m [torchaudio]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in ./.local/lib/python3.12/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (9.4.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.local/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.local/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.12/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.12/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# TWCC專用\n",
    "\n",
    "# 1. 卸載目前不相容的 PyTorch\n",
    "%pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# 2. 安裝官方穩定版 (支援 Tesla V100)\n",
    "# CUDA 12.1 或 12.4 的版本，目前最通用的指令\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c57d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "# 設定 Log\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a855b422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Corpus: ./data/FinanceRAG_corpus.parquet\n",
      "  - Queries: ./data/FinanceRAG_queries.parquet\n",
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_dir = \"./data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "corpus_filename = \"FinanceRAG_corpus.parquet\"\n",
    "queries_filename = \"FinanceRAG_queries.parquet\"\n",
    "\n",
    "corpus_path = os.path.join(data_dir, corpus_filename)\n",
    "queries_path = os.path.join(data_dir, queries_filename)\n",
    "\n",
    "if os.path.exists(corpus_path) and os.path.exists(queries_path):\n",
    "    print(f\"  - Corpus: {corpus_path}\")\n",
    "    print(f\"  - Queries: {queries_path}\")\n",
    "\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ec46b",
   "metadata": {},
   "source": [
    "# Define Core Classes \n",
    "(Loader, Encoder, Retriever, Reranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85eb9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import hashlib\n",
    "import heapq\n",
    "import json\n",
    "import logging\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union, cast\n",
    "\n",
    "import numpy as np\n",
    "from datasets import Dataset, Value, load_dataset\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "\n",
    "# 1. DataLoader ---\n",
    "class HFDataLoader:\n",
    "    def __init__(self, hf_repo: Optional[str] = None, subset: Optional[str] = None, keep_in_memory: bool = False, **kwargs):\n",
    "        self.corpus: Optional[Dataset] = None\n",
    "        self.queries: Optional[Dataset] = None\n",
    "        self.hf_repo = hf_repo\n",
    "        self.subset = subset\n",
    "        self.keep_in_memory = keep_in_memory\n",
    "        self.streaming = False\n",
    "\n",
    "    def load(self) -> Tuple[Dataset, Dataset]:\n",
    "        return self.corpus, self.queries\n",
    "\n",
    "# --- 2. Abstract Base Classes ---\n",
    "class Encoder(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def encode_queries(self, queries: List[str], **kwargs) -> Union[torch.Tensor, np.ndarray]: raise NotImplementedError\n",
    "    @abc.abstractmethod\n",
    "    def encode_corpus(self, corpus: Union[List[Dict], Dict], **kwargs) -> Union[torch.Tensor, np.ndarray]: raise NotImplementedError\n",
    "\n",
    "class Retrieval(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def retrieve(self, corpus, queries, top_k, **kwargs) -> Dict[str, Dict[str, float]]: raise NotImplementedError\n",
    "\n",
    "class Reranker(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def rerank(self, corpus, queries, results, top_k, **kwargs) -> Dict[str, Dict[str, float]]: raise NotImplementedError\n",
    "\n",
    "# --- 3. Concrete Implementations ---\n",
    "\n",
    "# SentenceTransformer Encoder\n",
    "class SentenceTransformerEncoder(Encoder):\n",
    "    def __init__(self, model_name_or_path: str, query_prompt: Optional[str] = None, doc_prompt: Optional[str] = None, **kwargs):\n",
    "        self.q_model = SentenceTransformer(model_name_or_path, **kwargs)\n",
    "        self.doc_model = self.q_model\n",
    "        self.query_prompt = query_prompt\n",
    "        self.doc_prompt = doc_prompt\n",
    "\n",
    "    def encode_queries(self, queries: List[str], batch_size: int = 16, **kwargs) -> Union[np.ndarray, Tensor]:\n",
    "        if self.query_prompt:\n",
    "            queries = [self.query_prompt + query for query in queries]\n",
    "        return self.q_model.encode(queries, batch_size=batch_size, **kwargs)\n",
    "\n",
    "    def encode_corpus(self, corpus: Union[List[Dict], Dict], batch_size: int = 8, **kwargs) -> Union[np.ndarray, Tensor]:\n",
    "        if isinstance(corpus, dict):\n",
    "            sentences = [((corpus[\"title\"][i] + \" \" + corpus[\"text\"][i]).strip() if \"title\" in corpus else corpus[\"text\"][i].strip()) for i in range(len(corpus[\"text\"]))]\n",
    "        else:\n",
    "            sentences = [((doc[\"title\"] + \" \" + doc[\"text\"]).strip() if \"title\" in doc else doc[\"text\"].strip()) for doc in corpus]\n",
    "        if self.doc_prompt:\n",
    "            sentences = [self.doc_prompt + s for s in sentences]\n",
    "        return self.doc_model.encode(sentences, batch_size=batch_size, **kwargs)\n",
    "\n",
    "# Dense Retrieval\n",
    "class DenseRetrieval(Retrieval):\n",
    "    def __init__(self, model: Encoder, batch_size: int = 64, corpus_chunk_size: int = 50000):\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.corpus_chunk_size = corpus_chunk_size\n",
    "\n",
    "    def retrieve(self, corpus, queries, top_k=100, score_function=\"cos_sim\", **kwargs):\n",
    "        logger.info(\"Encoding queries...\")\n",
    "        query_ids = list(queries.keys())\n",
    "        query_texts = [queries[qid] for qid in queries]\n",
    "        query_embeddings = self.model.encode_queries(query_texts, batch_size=self.batch_size, **kwargs)\n",
    "        \n",
    "        logger.info(\"Encoding corpus and searching...\")\n",
    "        sorted_corpus_ids = sorted(corpus, key=lambda k: len(corpus[k].get(\"title\", \"\") + corpus[k].get(\"text\", \"\")), reverse=True)\n",
    "        corpus_list = [corpus[cid] for cid in sorted_corpus_ids]\n",
    "        \n",
    "        self.results = {qid: {} for qid in query_ids}\n",
    "        result_heaps = {qid: [] for qid in query_ids}\n",
    "\n",
    "        # Batch process corpus\n",
    "        for start_idx in range(0, len(corpus), self.corpus_chunk_size):\n",
    "            end_idx = min(start_idx + self.corpus_chunk_size, len(corpus_list))\n",
    "            sub_corpus_embeddings = self.model.encode_corpus(corpus_list[start_idx:end_idx], batch_size=self.batch_size, **kwargs)\n",
    "            \n",
    "            # Convert to tensors if numpy\n",
    "            if isinstance(query_embeddings, np.ndarray): query_embeddings = torch.from_numpy(query_embeddings)\n",
    "            if isinstance(sub_corpus_embeddings, np.ndarray): sub_corpus_embeddings = torch.from_numpy(sub_corpus_embeddings)\n",
    "            \n",
    "            # Ensure device\n",
    "            if torch.cuda.is_available():\n",
    "                query_embeddings = query_embeddings.cuda()\n",
    "                sub_corpus_embeddings = sub_corpus_embeddings.cuda()\n",
    "\n",
    "            # Compute Similarity (Cosine)\n",
    "            # Normalize for Cosine Similarity\n",
    "            q_norm = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
    "            c_norm = torch.nn.functional.normalize(sub_corpus_embeddings, p=2, dim=1)\n",
    "            cos_scores = torch.mm(q_norm, c_norm.transpose(0, 1))\n",
    "            cos_scores[torch.isnan(cos_scores)] = -1\n",
    "\n",
    "            # Top-K collection\n",
    "            cos_scores = cos_scores.cpu() # Move back to CPU for heap operations\n",
    "            values, indices = torch.topk(cos_scores, min(top_k+1, cos_scores.size(1)), dim=1)\n",
    "            \n",
    "            values = values.tolist()\n",
    "            indices = indices.tolist()\n",
    "\n",
    "            for i, qid in enumerate(query_ids):\n",
    "                for score, idx in zip(values[i], indices[i]):\n",
    "                    doc_id = sorted_corpus_ids[start_idx + idx]\n",
    "                    if doc_id != qid: # Avoid self-retrieval if IDs match\n",
    "                         # Maintain heap\n",
    "                        if len(result_heaps[qid]) < top_k:\n",
    "                            heapq.heappush(result_heaps[qid], (score, doc_id))\n",
    "                        else:\n",
    "                            heapq.heappushpop(result_heaps[qid], (score, doc_id))\n",
    "\n",
    "        # Finalize results\n",
    "        for qid in result_heaps:\n",
    "            for score, doc_id in result_heaps[qid]:\n",
    "                self.results[qid][doc_id] = score\n",
    "        return self.results\n",
    "\n",
    "# Cross Encoder Reranker\n",
    "class CrossEncoderReranker(Reranker):\n",
    "    def __init__(self, model: CrossEncoder):\n",
    "        self.model = model\n",
    "\n",
    "    def rerank(self, corpus, queries, results, top_k, batch_size=32, **kwargs):\n",
    "        sentence_pairs, pair_ids = [], []\n",
    "        for query_id in results:\n",
    "            # Sort current results and take top_k\n",
    "            sorted_docs = sorted(results[query_id].items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "            for doc_id, _ in sorted_docs:\n",
    "                pair_ids.append([query_id, doc_id])\n",
    "                corpus_text = (corpus[doc_id].get(\"title\", \"\") + \" \" + corpus[doc_id].get(\"text\", \"\")).strip()\n",
    "                sentence_pairs.append([queries[query_id], corpus_text])\n",
    "\n",
    "        logger.info(f\"Starting Reranking for {len(sentence_pairs)} pairs...\")\n",
    "        scores = self.model.predict(sentence_pairs, batch_size=batch_size, **kwargs)\n",
    "        \n",
    "        reranked_results = {qid: {} for qid in results}\n",
    "        for (qid, doc_id), score in zip(pair_ids, scores):\n",
    "            reranked_results[qid][doc_id] = float(score)\n",
    "            \n",
    "        return reranked_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ef8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Task Definitions (FinDER)\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import Literal\n",
    "from datasets import load_dataset\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "\n",
    "class TaskMetadata(BaseModel):\n",
    "    name: str\n",
    "    dataset: dict\n",
    "    description: str = \"\"\n",
    "    type: str = \"RAG\"\n",
    "    domains: list = []\n",
    "\n",
    "class BaseTask:\n",
    "    def __init__(self, metadata: TaskMetadata):\n",
    "        self.metadata = metadata\n",
    "        self.queries = None\n",
    "        self.corpus = None\n",
    "        self.retrieve_results = None\n",
    "        self.rerank_results = None\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        pass\n",
    "\n",
    "    def retrieve(self, retriever, top_k=100, **kwargs):\n",
    "        self.retrieve_results = retriever.retrieve(self.corpus, self.queries, top_k=top_k, **kwargs)\n",
    "        return self.retrieve_results\n",
    "\n",
    "    def rerank(self, reranker, results=None, top_k=100, batch_size=32, **kwargs):\n",
    "        if results is None: results = self.retrieve_results\n",
    "        self.rerank_results = reranker.rerank(self.corpus, self.queries, results, top_k, batch_size, **kwargs)\n",
    "        return self.rerank_results\n",
    "    \n",
    "    def save_results(self, output_dir: str, top_k: int = 10):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        csv_path = os.path.join(output_dir, \"results.csv\")\n",
    "        final_results = self.rerank_results if self.rerank_results else self.retrieve_results\n",
    "        \n",
    "        with open(csv_path, \"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"query_id\", \"corpus_id\"])\n",
    "            # 寫入 CSV\n",
    "            for qid, docs in final_results.items():\n",
    "                sorted_docs = sorted(docs.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "                for doc_id, _ in sorted_docs:\n",
    "                    writer.writerow([qid, doc_id])\n",
    "        logger.info(f\"Saved results to {csv_path}\")\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FinDER(BaseTask):\n",
    "    def __init__(self, data_folder=\"./data\"):\n",
    "        self.metadata = TaskMetadata(\n",
    "            name=\"FinDER\",\n",
    "            description=\"Local Parquet Data\",\n",
    "            dataset={\n",
    "                \"path\": \"Local_Files\", \n",
    "                \"subset\": \"default\",\n",
    "            },\n",
    "            type=\"RAG\",\n",
    "            domains=[\"Report\"],\n",
    "        )\n",
    "        self.data_folder = data_folder\n",
    "        super().__init__(self.metadata)\n",
    "\n",
    "    def load_data(self):\n",
    "        # 設定檔案路徑\n",
    "        corpus_file = os.path.join(self.data_folder, \"FinanceRAG_corpus.parquet\")\n",
    "        queries_file = os.path.join(self.data_folder, \"FinanceRAG_queries.parquet\")\n",
    "\n",
    "        print(f\"Loading data from Parquet files in: {self.data_folder} ...\")\n",
    "        # 1. 讀取 Corpus\n",
    "        print(\"Loading Corpus (Parquet)...\")\n",
    "        # split=\"train\" 指定 split 直接拿 Dataset\n",
    "        corpus_ds = load_dataset(\"parquet\", data_files=corpus_file, split=\"train\")\n",
    "        \n",
    "        self.corpus = {}\n",
    "        for row in corpus_ds:\n",
    "            # FinanceRAG parquet: _id, title, text\n",
    "            doc_id = str(row.get(\"_id\"))\n",
    "            self.corpus[doc_id] = {\n",
    "                \"title\": row.get(\"title\", \"\"),\n",
    "                \"text\": row.get(\"text\", \"\")\n",
    "            }\n",
    "            \n",
    "        # 2. 讀取 Queries\n",
    "        print(\"Loading Queries (Parquet)...\")\n",
    "        queries_ds = load_dataset(\"parquet\", data_files=queries_file, split=\"train\")\n",
    "        \n",
    "        self.queries = {}\n",
    "        for row in queries_ds:\n",
    "            q_id = str(row.get(\"_id\"))\n",
    "            # Queries parquet\n",
    "            self.queries[q_id] = row.get(\"text\", \"\")\n",
    "\n",
    "        print(f\"Successfully loaded {len(self.corpus)} docs and {len(self.queries)} queries.\")\n",
    "        \n",
    "        # 驗證 ID\n",
    "        if self.corpus:\n",
    "            first_id = next(iter(self.corpus))\n",
    "            print(f\"Example Corpus ID check: {first_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba90c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading data from Parquet files in: ./data ...\n",
      "Loading Corpus (Parquet)...\n",
      "Loading Queries (Parquet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: intfloat/e5-large-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 13863 docs and 216 queries.\n",
      "Example Corpus ID check: ADBE20230004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Encoding queries...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Retrieval ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc08e4b7c28a459387887910bccd7259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Encoding corpus and searching...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5695fadd46b8450791a8e08ec0aedf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval done. Top 5 docs for query q00001:\n",
      "  1. MSFT20230966 (Score: 0.8740)\n",
      "  2. MSFT20230216 (Score: 0.8646)\n",
      "  3. MSFT20230015 (Score: 0.8594)\n",
      "  4. MSFT20230254 (Score: 0.8580)\n",
      "  5. MSFT20230155 (Score: 0.8534)\n",
      "\n",
      "--- Starting Reranking ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting Reranking for 21600 pairs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59cbd4094fe4b98989e30ed1136165a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/338 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4: Execution Pipeline\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. Initialize Task & Data\n",
    "finder_task = FinDER(data_folder=\"./data\")\n",
    "\n",
    "# 2. Initialize Retrieval Model (Bi-Encoder)\n",
    "# 使用 e5-large-v2\n",
    "encoder_model = SentenceTransformerEncoder(\n",
    "    model_name_or_path='intfloat/e5-large-v2',\n",
    "    query_prompt='query: ',\n",
    "    doc_prompt='passage: ',\n",
    "    device=device \n",
    ")\n",
    "\n",
    "retrieval_model = DenseRetrieval(\n",
    "    model=encoder_model,\n",
    "    batch_size=128\n",
    ")\n",
    "\n",
    "# 3. Perform Retrieval\n",
    "print(\"\\n--- Starting Retrieval ---\")\n",
    "retrieval_result = finder_task.retrieve(\n",
    "    retriever=retrieval_model,\n",
    "    top_k=100\n",
    ")\n",
    "\n",
    "# Print Example\n",
    "first_qid = list(retrieval_result.keys())[0]\n",
    "print(f\"Retrieval done. Top 5 docs for query {first_qid}:\")\n",
    "top_docs = sorted(retrieval_result[first_qid].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for i, (doc_id, score) in enumerate(top_docs):\n",
    "    print(f\"  {i+1}. {doc_id} (Score: {score:.4f})\")\n",
    "\n",
    "# 4. Initialize Reranker (Cross-Encoder)\n",
    "print(\"\\n--- Starting Reranking ---\")\n",
    "reranker_model = CrossEncoderReranker(\n",
    "    model=CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', device=device)\n",
    ")\n",
    "\n",
    "# 5. Perform Reranking\n",
    "reranking_result = finder_task.rerank(\n",
    "    reranker=reranker_model,\n",
    "    results=retrieval_result,\n",
    "    top_k=100,\n",
    "    batch_size=64 \n",
    ")\n",
    "\n",
    "# Print Example\n",
    "print(f\"Reranking done. Top 5 docs for query {first_qid}:\")\n",
    "top_rerank_docs = sorted(reranking_result[first_qid].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "for i, (doc_id, score) in enumerate(top_rerank_docs):\n",
    "    print(f\"  {i+1}. {doc_id} (Score: {score:.4f})\")\n",
    "\n",
    "# 6. Save Results\n",
    "output_dir = './results'\n",
    "finder_task.save_results(output_dir=output_dir)\n",
    "print(f\"\\nResults saved to {output_dir}/results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547e8f8-def5-4754-9796-fc2c4452c36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
