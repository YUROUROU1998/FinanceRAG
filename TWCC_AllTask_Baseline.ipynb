{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AllTask Baseline for FinanceRAG Tasks\n",
                "\n",
                "loads data from the local `data`, runs a retrieval and reranking pipeline, and aggregates the results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8dda98c0",
            "metadata": {},
            "outputs": [],
            "source": [
                "#TWCC專用\n",
                "%pip install sentence-transformers datasets pytrec_eval accelerate pandas pyarrow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f89f5cf1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# TWCC專用\n",
                "\n",
                "# 1. 卸載目前不相容的 PyTorch\n",
                "%pip uninstall -y torch torchvision torchaudio\n",
                "\n",
                "# 2. 安裝官方穩定版 (支援 Tesla V100)\n",
                "# CUDA 12.1 或 12.4 的版本，目前最通用的指令\n",
                "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
                "\n",
                "%pip install ipywidgets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "784bdbd2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "import os\n",
                "import csv\n",
                "import json\n",
                "import heapq\n",
                "import abc\n",
                "from pathlib import Path\n",
                "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union, cast\n",
                "\n",
                "import numpy as np\n",
                "import torch\n",
                "from datasets import Dataset, Value, load_dataset\n",
                "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
                "from pydantic import BaseModel, Field\n",
                "from tqdm.auto import tqdm\n",
                "import pandas as pd\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e48db324",
            "metadata": {},
            "source": [
                "## Core Classes (DataLoader, Encoder, Retrieval, Reranker)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c46616ea",
            "metadata": {},
            "outputs": [],
            "source": [
                "class HFDataLoader:\n",
                "    def __init__(\n",
                "            self,\n",
                "            data_folder: str,\n",
                "            subset: str,\n",
                "            corpus_file: str,\n",
                "            query_file: str,\n",
                "            keep_in_memory: bool = False,\n",
                "    ):\n",
                "        self.corpus: Optional[Dataset] = None\n",
                "        self.queries: Optional[Dataset] = None\n",
                "        self.data_folder = data_folder\n",
                "        self.subset = subset\n",
                "        self.corpus_file = os.path.join(data_folder, corpus_file)\n",
                "        self.query_file = os.path.join(data_folder, query_file)\n",
                "        self.keep_in_memory = keep_in_memory\n",
                "\n",
                "    def load(self) -> Tuple[Dataset, Dataset]:\n",
                "        if self.corpus is None:\n",
                "            logger.info(f\"Loading Corpus from {self.corpus_file}...\")\n",
                "            if self.corpus_file.endswith(\".parquet\"):\n",
                "                 self.corpus = load_dataset(\"parquet\", data_files=self.corpus_file, split=\"train\", keep_in_memory=self.keep_in_memory)\n",
                "            else:\n",
                "                self.corpus = load_dataset(\"json\", data_files=self.corpus_file, split=\"train\", keep_in_memory=self.keep_in_memory)\n",
                "            \n",
                "            # Standardize columns\n",
                "            if \"_id\" in self.corpus.column_names:\n",
                "                self.corpus = self.corpus.cast_column(\"_id\", Value(\"string\"))\n",
                "                self.corpus = self.corpus.rename_column(\"_id\", \"id\")\n",
                "            \n",
                "            # Keep only necessary columns\n",
                "            keep_cols = [\"id\", \"text\", \"title\"]\n",
                "            self.corpus = self.corpus.remove_columns([c for c in self.corpus.column_names if c not in keep_cols])\n",
                "            \n",
                "        if self.queries is None:\n",
                "            logger.info(f\"Loading Queries from {self.query_file}...\")\n",
                "            if self.query_file.endswith(\".parquet\"):\n",
                "                self.queries = load_dataset(\"parquet\", data_files=self.query_file, split=\"train\", keep_in_memory=self.keep_in_memory)\n",
                "            else:\n",
                "                self.queries = load_dataset(\"json\", data_files=self.query_file, split=\"train\", keep_in_memory=self.keep_in_memory)\n",
                "            \n",
                "            if \"_id\" in self.queries.column_names:\n",
                "                self.queries = self.queries.cast_column(\"_id\", Value(\"string\"))\n",
                "                self.queries = self.queries.rename_column(\"_id\", \"id\")\n",
                "            \n",
                "            keep_cols = [\"id\", \"text\"]\n",
                "            self.queries = self.queries.remove_columns([c for c in self.queries.column_names if c not in keep_cols])\n",
                "\n",
                "        return self.corpus, self.queries\n",
                "\n",
                "class Encoder(abc.ABC):\n",
                "    @abc.abstractmethod\n",
                "    def encode_queries(self, queries: List[str], **kwargs) -> Union[torch.Tensor, np.ndarray]: raise NotImplementedError\n",
                "    @abc.abstractmethod\n",
                "    def encode_corpus(self, corpus: Union[List[Dict], Dict], **kwargs) -> Union[torch.Tensor, np.ndarray]: raise NotImplementedError\n",
                "\n",
                "class Retrieval(abc.ABC):\n",
                "    @abc.abstractmethod\n",
                "    def retrieve(self, corpus, queries, top_k, **kwargs) -> Dict[str, Dict[str, float]]: raise NotImplementedError\n",
                "\n",
                "class Reranker(abc.ABC):\n",
                "    @abc.abstractmethod\n",
                "    def rerank(self, corpus, queries, results, top_k, **kwargs) -> Dict[str, Dict[str, float]]: raise NotImplementedError\n",
                "\n",
                "class SentenceTransformerEncoder(Encoder):\n",
                "    def __init__(self, model_name_or_path: str, query_prompt: str = None, doc_prompt: str = None, **kwargs):\n",
                "        self.q_model = SentenceTransformer(model_name_or_path, **kwargs)\n",
                "        self.doc_model = self.q_model\n",
                "        self.query_prompt = query_prompt\n",
                "        self.doc_prompt = doc_prompt\n",
                "\n",
                "    def encode_queries(self, queries: List[str], batch_size: int = 16, **kwargs):\n",
                "        if self.query_prompt:\n",
                "            queries = [self.query_prompt + q for q in queries]\n",
                "        return self.q_model.encode(queries, batch_size=batch_size, **kwargs)\n",
                "\n",
                "    def encode_corpus(self, corpus: Union[List[Dict], Dict], batch_size: int = 8, **kwargs):\n",
                "        if isinstance(corpus, dict):\n",
                "            sentences = [(corpus[\"title\"][i] + \" \" + corpus[\"text\"][i]).strip() if \"title\" in corpus else corpus[\"text\"][i].strip() for i in range(len(corpus[\"text\"]))]\n",
                "        else:\n",
                "            sentences = [(doc.get(\"title\", \"\") + \" \" + doc[\"text\"]).strip() for doc in corpus]\n",
                "        if self.doc_prompt:\n",
                "            sentences = [self.doc_prompt + s for s in sentences]\n",
                "        return self.doc_model.encode(sentences, batch_size=batch_size, **kwargs)\n",
                "\n",
                "class DenseRetrieval(Retrieval):\n",
                "    def __init__(self, model: Encoder, batch_size: int = 64, corpus_chunk_size: int = 50000):\n",
                "        self.model = model\n",
                "        self.batch_size = batch_size\n",
                "        self.corpus_chunk_size = corpus_chunk_size\n",
                "\n",
                "    def retrieve(self, corpus, queries, top_k=100, score_function=\"cos_sim\", **kwargs):\n",
                "        logger.info(\"Encoding queries...\")\n",
                "        query_ids = list(queries.keys())\n",
                "        query_texts = [queries[qid] for qid in queries]\n",
                "        query_embeddings = self.model.encode_queries(query_texts, batch_size=self.batch_size, **kwargs)\n",
                "        \n",
                "        logger.info(\"Encoding corpus and searching...\")\n",
                "        sorted_corpus_ids = sorted(corpus, key=lambda k: len(corpus[k].get(\"title\", \"\") + corpus[k].get(\"text\", \"\")), reverse=True)\n",
                "        corpus_list = [corpus[cid] for cid in sorted_corpus_ids]\n",
                "        \n",
                "        self.results = {qid: {} for qid in query_ids}\n",
                "        result_heaps = {qid: [] for qid in query_ids}\n",
                "\n",
                "        for start_idx in tqdm(range(0, len(corpus), self.corpus_chunk_size), desc=\"Retrieving Chunks\"):\n",
                "            end_idx = min(start_idx + self.corpus_chunk_size, len(corpus_list))\n",
                "            sub_corpus_embeddings = self.model.encode_corpus(corpus_list[start_idx:end_idx], batch_size=self.batch_size, **kwargs)\n",
                "            \n",
                "            if isinstance(query_embeddings, np.ndarray): query_embeddings = torch.from_numpy(query_embeddings)\n",
                "            if isinstance(sub_corpus_embeddings, np.ndarray): sub_corpus_embeddings = torch.from_numpy(sub_corpus_embeddings)\n",
                "            \n",
                "            if torch.cuda.is_available():\n",
                "                query_embeddings = query_embeddings.cuda()\n",
                "                sub_corpus_embeddings = sub_corpus_embeddings.cuda()\n",
                "\n",
                "            q_norm = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
                "            c_norm = torch.nn.functional.normalize(sub_corpus_embeddings, p=2, dim=1)\n",
                "            cos_scores = torch.mm(q_norm, c_norm.transpose(0, 1))\n",
                "            cos_scores[torch.isnan(cos_scores)] = -1\n",
                "            cos_scores = cos_scores.cpu()\n",
                "\n",
                "            values, indices = torch.topk(cos_scores, min(top_k+1, cos_scores.size(1)), dim=1)\n",
                "            values, indices = values.tolist(), indices.tolist()\n",
                "\n",
                "            for i, qid in enumerate(query_ids):\n",
                "                for score, idx in zip(values[i], indices[i]):\n",
                "                    doc_id = sorted_corpus_ids[start_idx + idx]\n",
                "                    if doc_id != qid:\n",
                "                        if len(result_heaps[qid]) < top_k:\n",
                "                            heapq.heappush(result_heaps[qid], (score, doc_id))\n",
                "                        else:\n",
                "                            heapq.heappushpop(result_heaps[qid], (score, doc_id))\n",
                "\n",
                "        for qid in result_heaps:\n",
                "            for score, doc_id in result_heaps[qid]:\n",
                "                self.results[qid][doc_id] = score\n",
                "        return self.results\n",
                "\n",
                "class CrossEncoderReranker(Reranker):\n",
                "    def __init__(self, model: CrossEncoder):\n",
                "        self.model = model\n",
                "\n",
                "    def rerank(self, corpus, queries, results, top_k, batch_size=32, **kwargs):\n",
                "        sentence_pairs, pair_ids = [], []\n",
                "        for query_id in results:\n",
                "            sorted_docs = sorted(results[query_id].items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
                "            for doc_id, _ in sorted_docs:\n",
                "                pair_ids.append([query_id, doc_id])\n",
                "                corpus_text = (corpus[doc_id].get(\"title\", \"\") + \" \" + corpus[doc_id].get(\"text\", \"\")).strip()\n",
                "                sentence_pairs.append([queries[query_id], corpus_text])\n",
                "\n",
                "        logger.info(f\"Starting Reranking for {len(sentence_pairs)} pairs...\")\n",
                "        scores = self.model.predict(sentence_pairs, batch_size=batch_size, show_progress_bar=True, **kwargs)\n",
                "        \n",
                "        reranked_results = {qid: {} for qid in results}\n",
                "        for (qid, doc_id), score in zip(pair_ids, scores):\n",
                "            reranked_results[qid][doc_id] = float(score)\n",
                "        return reranked_results"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4e19dc7c",
            "metadata": {},
            "source": [
                "## Task Definitions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d6d34dc3",
            "metadata": {},
            "outputs": [],
            "source": [
                "class TaskMetadata(BaseModel):\n",
                "    name: str\n",
                "    dataset: dict\n",
                "    description: str = \"\"\n",
                "\n",
                "class BaseTask:\n",
                "    def __init__(self, metadata: TaskMetadata, data_folder: str = \"./data\"):\n",
                "        self.metadata = metadata\n",
                "        self.data_folder = data_folder\n",
                "        self.queries = None\n",
                "        self.corpus = None\n",
                "        self.retrieve_results = None\n",
                "        self.rerank_results = None\n",
                "        self.qrels = None\n",
                "        self.load_data()\n",
                "\n",
                "    def load_data(self):\n",
                "        subset = self.metadata.dataset[\"subset\"]\n",
                "        # Determine folder name based on subset\n",
                "        folder_name = subset.lower()\n",
                "        if subset == \"MultiHiertt\":\n",
                "            folder_name = \"multiheirtt\"\n",
                "        \n",
                "        if subset == 'FinDER':\n",
                "            corpus_file = \"FinanceRAG_corpus.parquet\"\n",
                "            query_file = \"FinanceRAG_queries.parquet\"\n",
                "        else:\n",
                "            corpus_file = f\"{folder_name}_corpus.jsonl/corpus.jsonl\"\n",
                "            query_file = f\"{folder_name}_queries.jsonl/queries.jsonl\"\n",
                "        \n",
                "        loader = HFDataLoader(\n",
                "            data_folder=self.data_folder,\n",
                "            subset=subset,\n",
                "            corpus_file=corpus_file,\n",
                "            query_file=query_file\n",
                "        )\n",
                "        corpus, queries = loader.load()\n",
                "        \n",
                "        self.queries = {row[\"id\"]: row[\"text\"] for row in queries}\n",
                "        self.corpus = {row[\"id\"]: {\"title\": row.get(\"title\", \"\"), \"text\": row.get(\"text\", \"\")} for row in corpus}\n",
                "        logger.info(f\"Loaded {len(self.corpus)} docs and {len(self.queries)} queries for {subset}.\")\n",
                "        \n",
                "        # Load Qrels\n",
                "        qrels_file = os.path.join(self.data_folder, f\"{subset}_qrels.tsv\")\n",
                "        if os.path.exists(qrels_file):\n",
                "            logger.info(f\"Loading qrels from {qrels_file}...\")\n",
                "            self.qrels = pd.read_csv(qrels_file, sep='\\t')\n",
                "            # Ensure columns are strings for merging\n",
                "            self.qrels['query_id'] = self.qrels['query_id'].astype(str)\n",
                "            self.qrels['corpus_id'] = self.qrels['corpus_id'].astype(str)\n",
                "\n",
                "    def retrieve(self, retriever, top_k=100, **kwargs):\n",
                "        self.retrieve_results = retriever.retrieve(self.corpus, self.queries, top_k=top_k, **kwargs)\n",
                "        return self.retrieve_results\n",
                "\n",
                "    def rerank(self, reranker, results=None, top_k=100, batch_size=32, **kwargs):\n",
                "        if results is None: results = self.retrieve_results\n",
                "        self.rerank_results = reranker.rerank(self.corpus, self.queries, results, top_k, batch_size, **kwargs)\n",
                "        return self.rerank_results\n",
                "    \n",
                "    def evaluate(self, results, top_k=10):\n",
                "        if self.qrels is None:\n",
                "            return None\n",
                "        \n",
                "        # Convert results to DataFrame for easier evaluation\n",
                "        res_rows = []\n",
                "        for qid, docs in results.items():\n",
                "            sorted_docs = sorted(docs.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
                "            for doc_id, score in sorted_docs:\n",
                "                res_rows.append({'query_id': str(qid), 'corpus_id': str(doc_id)})\n",
                "        \n",
                "        df_res = pd.DataFrame(res_rows)\n",
                "        \n",
                "        # Merge with qrels to find hits\n",
                "        # We check if (query_id, corpus_id) pair exists in qrels\n",
                "        merged = df_res.merge(self.qrels, on=['query_id', 'corpus_id'], how='inner')\n",
                "        \n",
                "        # Calculate Accuracy (at least one correct document retrieved per query)\n",
                "        # Note: This is a simplified metric based on the user's code1.ipynb logic\n",
                "        unique_queries_with_hits = merged['query_id'].nunique()\n",
                "        total_queries = len(results)\n",
                "        accuracy = unique_queries_with_hits / total_queries if total_queries > 0 else 0.0\n",
                "        \n",
                "        return {'accuracy': accuracy, 'hits': unique_queries_with_hits, 'total_queries': total_queries}\n",
                "\n",
                "class FinDER(BaseTask):\n",
                "    def __init__(self, data_folder=\"./data\"):\n",
                "        metadata = TaskMetadata(name=\"FinDER\", dataset={\"subset\": \"FinDER\"})\n",
                "        super().__init__(metadata, data_folder)\n",
                "\n",
                "class FinQABench(BaseTask):\n",
                "    def __init__(self, data_folder=\"./data\"):\n",
                "        metadata = TaskMetadata(name=\"FinQABench\", dataset={\"subset\": \"FinQABench\"})\n",
                "        super().__init__(metadata, data_folder)\n",
                "\n",
                "class FinQA(BaseTask):\n",
                "    def __init__(self, data_folder=\"./data\"):\n",
                "        metadata = TaskMetadata(name=\"FinQA\", dataset={\"subset\": \"FinQA\"})\n",
                "        super().__init__(metadata, data_folder)\n",
                "\n",
                "class FinanceBench(BaseTask):\n",
                "    def __init__(self, data_folder=\"./data\"):\n",
                "        metadata = TaskMetadata(name=\"FinanceBench\", dataset={\"subset\": \"FinanceBench\"})\n",
                "        super().__init__(metadata, data_folder)\n",
                "\n",
                "class ConvFinQA(BaseTask):\n",
                "    def __init__(self, data_folder=\"./data\"):\n",
                "        metadata = TaskMetadata(name=\"ConvFinQA\", dataset={\"subset\": \"ConvFinQA\"})\n",
                "        super().__init__(metadata, data_folder)\n",
                "\n",
                "class MultiHiertt(BaseTask):\n",
                "    def __init__(self, data_folder=\"./data\"):\n",
                "        metadata = TaskMetadata(name=\"MultiHiertt\", dataset={\"subset\": \"MultiHiertt\"})\n",
                "        super().__init__(metadata, data_folder)\n",
                "\n",
                "class TATQA(BaseTask):\n",
                "    def __init__(self, data_folder=\"./data\"):\n",
                "        metadata = TaskMetadata(name=\"TATQA\", dataset={\"subset\": \"TATQA\"})\n",
                "        super().__init__(metadata, data_folder)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac6aa477",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Execution Pipeline\n",
                "import gc\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Initialize Models\n",
                "encoder_model = SentenceTransformerEncoder(\n",
                "    model_name_or_path='intfloat/e5-large-v2',\n",
                "    query_prompt='query: ',\n",
                "    doc_prompt='passage: ',\n",
                "    device=device\n",
                ")\n",
                "retrieval_model = DenseRetrieval(model=encoder_model, batch_size=128)\n",
                "\n",
                "reranker_model = CrossEncoderReranker(\n",
                "    model=CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2', device=device)\n",
                ")\n",
                "\n",
                "# List of Tasks\n",
                "task_classes = [\n",
                "    FinDER,\n",
                "    FinQABench,\n",
                "    FinQA,\n",
                "    FinanceBench,\n",
                "    ConvFinQA,\n",
                "    MultiHiertt,\n",
                "    TATQA\n",
                "]\n",
                "\n",
                "all_results = []\n",
                "\n",
                "for TaskClass in tqdm(task_classes, desc=\"Processing Tasks\"):\n",
                "    try:\n",
                "        task = TaskClass(data_folder=\"./data\")\n",
                "        print(f\"\\n>>> Processing Task: {task.metadata.name} <<<\")\n",
                "        \n",
                "        # Retrieve\n",
                "        print(f\"--- Retrieving ({task.metadata.name}) ---\")\n",
                "        retrieve_result = task.retrieve(retrieval_model, top_k=100)\n",
                "        \n",
                "        # Rerank\n",
                "        print(f\"--- Reranking ({task.metadata.name}) ---\")\n",
                "        reranking_result = task.rerank(reranker_model, results=retrieve_result, top_k=100, batch_size=64)\n",
                "        \n",
                "        # Evaluate\n",
                "        print(f\"--- Evaluating ({task.metadata.name}) ---\")\n",
                "        if task.qrels is not None:\n",
                "            metrics = task.evaluate(reranking_result)\n",
                "            print(f\"Task {task.metadata.name} Metrics: {metrics}\")\n",
                "        else:\n",
                "            print(f\"No qrels found for {task.metadata.name}, skipping evaluation.\")\n",
                "\n",
                "        # Collect Results\n",
                "        print(f\"--- Collecting Results ({task.metadata.name}) ---\")\n",
                "        for query_id, result in reranking_result.items():\n",
                "            top_10 = sorted(result.items(), key=lambda x: x[1], reverse=True)[:10]\n",
                "            for corpus_id, score in top_10:\n",
                "                all_results.append({\n",
                "                    'query_id': query_id,\n",
                "                    'corpus_id': corpus_id,\n",
                "                    'score': score\n",
                "                })\n",
                "        \n",
                "        # Cleanup\n",
                "        del task\n",
                "        del retrieve_result\n",
                "        del reranking_result\n",
                "        gc.collect()\n",
                "        if torch.cuda.is_available():\n",
                "            torch.cuda.empty_cache()\n",
                "            \n",
                "    except Exception as e:\n",
                "        print(f\"Error processing task {TaskClass.__name__}: {e}\")\n",
                "        import traceback\n",
                "        traceback.print_exc()\n",
                "\n",
                "# Save Final Results\n",
                "print(\"\\n=== Saving Submission File ===\")\n",
                "output_file = 'submission.csv'\n",
                "with open(output_file, 'w', newline='') as f:\n",
                "    writer = csv.writer(f)\n",
                "    writer.writerow(['query_id', 'corpus_id'])\n",
                "    for row in all_results:\n",
                "        writer.writerow([row['query_id'], row['corpus_id']])\n",
                "print(f\"Results saved to {output_file}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
